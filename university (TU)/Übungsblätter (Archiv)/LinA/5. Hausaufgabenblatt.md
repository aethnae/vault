***
#### Aufgabe 5.1

**(i)**

Laut Voraussetzung ist $A \in R^{n,n}$ eine Matrix, für die ein $m \in \mathbb{N}$ mit $A^{m} = I_{n}$ existiert. Somit gilt

$$
A^{1} \ast A^{m-1} = I_{n}
$$

Aufgrund von Assoziativität in $R^{n,n}$ und somit Kommutativität von $A$ bezüglich $A$ gilt

$$
A \ast A^{m-1} = \underbrace{ (A) }_{ \text{ $1$ mal } } \ast \underbrace{ (A \ast A \ast \ldots \ast A) }_{ \text{ $m-1$ mal } } = \underbrace{ (A \ast A \ast \ldots \ast A) }_{ \text{ $m-1$ mal } } \ast \underbrace{ (A) }_{ \text{ $1$ mal } } = A^{m-1} \ast A
$$

Somit besitzt $A$ ein identisches Rechts- und Linksinverses, und ist somit mit $A^{m-1}$ invers.

**(ii)**

Die Menge $B \coloneqq \{ A^{k} \mid k \in \mathbb{N} \}$ umfasst die Matrix $A$ sowie ihre Vielfachen. Für die Bestimmung von $\lvert B \rvert$ ist relevant, dass Elemente, die mehrfach in der Menge vorkommen, **nicht** zur Mächtigkeit von $B$ beitragen.
Um zu zeigen, dass sich die Elemente aus $B$ für $k > m$ wiederholen, und somit ignoriert werden können, ist

$$
B = \{ A^{k} \mid k \in \mathbb{N} \} = \{ A^{k} \mid k \in \{ 1,2,\ldots,m \} \} \eqqcolon \hat{B}
$$

zu zeigen. Dafür ist die Inklusion, wie auch die umgekehrte Inklusion zu beweisen.

`\begin{proof}`
"${} \supseteq {}$":

Zu zeigen ist $\{ 1,2,\ldots,m \} \in \mathbb{N}$. Da $m \in \mathbb{N}$ per Voraussetzung gilt, gilt auch die Inklusion trivialerweise.

"${} \subseteq {}$":

Zu zeigen ist $A \in B \implies A \in \hat{B}$. Trivialerweise gilt für $k \leq m$, dass $A \in B \implies A \in \hat{B}$. Für $k > m$ kann $k$ geschrieben werden als

$$
k = xm + i, \; x \in \mathbb{N}, i \in \mathbb{N}, i < m
$$

Somit gilt

$$
\begin{align}
A^{xm + i} & = A^{xm} \ast A^{i} \\
 & = (A^{m})^{x} \ast A^{i} \\
 & = I_{n}^{x} \ast A^{i} \\
 & = I_{n} \ast A^{i} \\
 & = A^{i}
\end{align}
$$

dies gilt, da Vielfache der Einheitsmatrix wieder die Einheitsmatrix ergeben. Somit gilt $A^{i} \in \hat{B}$, da $i < m$, und dies der triviale Fall von oben war.

Somit ist $B = \hat{B}$. Um die Mächtigkeit zu bestimmen, bleibt zu zeigen, dass für alle $A \in B$ keine

$$
A^{l} = A^{k}, \quad l \neq k, \quad l,k \in \{ 1,2,\ldots,m \}
$$

existieren. D.h., es dürfen keine identischen Matrizen für unterschiedliche Anzahlen der Vervielfachung von $A$ in $B$ existieren.

Angenommen, $A^{l} = A^{k}$ für $l \neq k, \; l,k \in \{ 1,2,\ldots,m \}$ gilt. Somit gilt auch

$$
\begin{align}
A^{l} & = A^{k} \\
\Longleftrightarrow A^{m-l} \ast A^{l} & = A^{m-l} \ast A^{k} \\
\Longleftrightarrow A^{m} & = A^{m-l+k} \\
\Longleftrightarrow I_{n} & = A^{m} \ast A^{k - l} \\
\Longleftrightarrow I_{n} & = I_{n} \ast A^{k-l} \\
\Longleftrightarrow I_{n} & = A^{k-l} 
\end{align}
$$

Da $k,l \in \{ 1,2,\ldots,m \}$, gilt **höchstens**

$$
k - l = m-1
$$

Laut Voraussetzung ist $m$ jedoch die kleinste Zahl in $\mathbb{N}$ mit $A^{m} = I_{n}$. Somit besteht ein Widerspruch, und $A^{l} = A^{k} \implies l = k$.

`\end{proof}`
<br> 

Somit gilt abschließend, dass in $B$ keine Matrizen mehrmals vorkommen, und sich diese Matrizen ab $k > m$ wiederholen. Damit gilt

$$
\lvert B \rvert = m
$$

***
#### Aufgabe 5.2

**(i)**

`\begin{proof}`
Sei $p \coloneqq \sum_{i = 0}^{m}\alpha_{i}t^{i}$ und $q \coloneqq \sum_{i = 0}^{n}\beta_{i}t^{i}$. Wir definieren

$$
\hat{p} \coloneqq \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i}, \quad \hat{q} \coloneqq \sum_{i = 0}^{d} \hat{\beta}_{i}t^{i},
$$

wobei $d = \max(m,n)$, und

$$
\hat{\alpha} \coloneqq 
\begin{cases}
\alpha_{i} \quad , i \in \{ 0,1,2,\ldots,m \} \\
0 \quad , \text{ sonst }
\end{cases}, \quad 
\hat{\beta} \coloneqq 
\begin{cases}
\beta_{i} \quad , i \in \{ 0,1,2,\ldots,n \} \\
0 \quad , \text{ sonst }
\end{cases}
$$

Laut Definition der Polynome bezeichnen wir diese als *gleich*, wenn $\hat{\alpha}_{i} = \alpha_{i}$ für $i \in \{ 0,1,\ldots,d \}$ gilt. Für $d = m$ gilt dies laut der Definition von $\hat{\alpha}_{i}$. Für $d = n$ ist $\hat{\alpha}_{i} = 0$ für alle $i > m$. Addition eines Nullsummanden ändert nichts an $\hat{\alpha}_{i}$, somit gilt auch hier $\hat{\alpha}_{i} = \alpha_{i}$. Analog gilt dies auch für ${} \beta_{i}$, und somit gilt $p = \hat{p}$ wie auch $q = \hat{q}$.

Somit kann $p + q$ auch als $\hat{p} + \hat{q}$ geschrieben werden, und es gilt:

$$
\begin{align}
\hat{p} + \hat{q} & = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} + \sum_{i = 0}^{d} \hat{\beta}_{i}t^{i} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} + \hat{\beta}_{i}t^{i} \\
 & = \sum_{i = 0}^{d} t^{i}(\hat{\alpha}_{i}+\hat{\beta}_{i})
\end{align}
$$

Somit gilt für $f_{A}(p+q) = f_{A}(\hat{p} + \hat{q})$:

$$
f_{A}(\hat{p} + \hat{q}) = \sum_{i = 0}^{d} A^{i}(\hat{\alpha}_{i} + \hat{\beta}_{i})
$$

sowie $f_{A}(\hat{p}) = \sum_{i = 0}^{d}\hat{\alpha}_{i}A^{i}$ und $f_{A}(\hat{q}) = \sum_{i = 0}^{d}\hat{\beta}_{i}A^{i}$. Somit gilt für die Addition

$$
\begin{align}
f_{A}(p) + f_{A}(q) = f_{A}(\hat{p}) + f_{A}(\hat{q}) & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} + \sum_{i = 0}^{d} \hat{\beta}_{i}A^{i} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} + \hat{\beta}_{i}A^{i} \\
 & = \sum_{i = 0}^{d} A^{i}(\hat{\alpha}_{i} + \hat{\beta}_{i}) \\
 & = f_{A}(\hat{p} + \hat{q} = f_{A}(p+q)
\end{align}
$$

Für $pq$ gilt

$$
pq = \hat{p}\hat{q} = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} \cdot \sum_{j = 0}^{d} \hat{\beta}_{j}t^{j} = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}t^{i} \hat{\beta}_{j}t^{j}
$$

Somit gilt eingesetzt in unsere Abbildung:

$$
\begin{align}
f_{A}(pq) = f_{A}(\hat{p}\hat{q}) & = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \hat{\beta}_{j}A^{j} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \\
 & = f_{A}(\hat{p}) \ast f_{A}(\hat{q}) \\
 & = f_{A}(p) \ast f_{A}(q)
\end{align}
$$

`\end{proof}`
<br> 

**(ii)**

`\begin{proof}`
Seien $B,C \in f_{A}(R[t])$ fest, aber beliebig. Sei $C = f_{A}(p)$ und $B = f_{A}(q)$, somit gilt

$$
\begin{align}
CB = f_{A}(p) \ast f_{A}(q) = f_{A}(\hat{p}) \ast f_{A}(\hat{q}) & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \\
 & = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}A^{i}\hat{\beta}_{j}A^{j} \\
 & = \sum_{j = 0}^{d} \sum_{i = 0}^{d} \hat{\beta}_{j}A^{j}\hat{\alpha}_{i}A^{i} \\
 & = \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \ast \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \\
 & = f_{A}(\hat{q}) \ast f_{A}(\hat{p}) \\
 & = f_{A}(q) \ast f_{A}(p) \\
 & = BC
\end{align}
$$

`\end{proof}`

Damit $f_{A}$ surjektiv ist, muss 

$$
\forall \hat{A} \in R^{n,n} \; \exists \; p \in R[t] : f_{A}(p) = \hat{A}
$$
 gelten. Für $n = 1$ gilt $\hat{A} = [x]$. Nun kann also immer $p$ vom Grad $0$ gewählt werden, so dass

$$
f_{A}(p) = \alpha_{0}A^{0} = \alpha_{0}I_{1}
$$

gilt. Sicherlich ist in $R[t]$ auch ein $p$ mit $\alpha_{0} = x$ enthalten, sodass

$$
\alpha_{0}I_{1} = xI_{1} = [x]
$$

 gilt. $I_{1}$ ist für $1 \times 1$ Matrizen $1$, daher ergibt die Skalarmultiplikation mit $x$ die Matrix, die nur $x$ enthält. Damit ist ${} f_{A}$ für $n = 1$ surjektiv.

Angenommen, $f_{A}$ ist surjektiv für $n \geq 2$, so gilt aus $(ii)$

$$
B,C \in f_{A}(R[t]) \implies B,C \in R^{n,n},
$$

wobei $BC = CB$ gilt. Dies widerspricht jedoch der Nichtkommutativität der Matrizenmultiplikation, welche bekanntlich nur für $n = 1$ nicht gilt. Somit kann $f_{A}$ für $n \geq 2$ nicht surjektiv sein.

***
#### Aufgabe 5.3

Da $A^{-1} = A^{\mathrm{T}}$  gilt, gilt auch $A \ast A^{\mathrm{T}} = I_{n}$. Somit ist das inverse Element für $A$ eine untere Dreiecksmatrix, da die Nullelemente von $A$ durch Transposition getauscht werden.

Fängt man nun an, eine Matrix $A \in R^{n,n}$ mit ihrer Transponierten zu multiplizieren, um auf die Einheitsmatrix zu kommen, wird folgender elementarer Zusammenhang klar:

$$
A \ast A^{-1} =
\begin{bmatrix}
x_{1,1} & \ldots & x_{1,n} \\
\vdots & \ddots & \vdots \\
0 & \ldots & x_{n,n}
\end{bmatrix}
\ast
\begin{bmatrix}
x_{1,1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
x_{n,1} & \ldots & x_{n,n}
\end{bmatrix}
=
\begin{bmatrix}
1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & 1
\end{bmatrix}
= I_{n}
$$

wobei $A$ eine obere-, und $A^{-1}$ eine untere Dreiecksmatrix ist. Somit gilt für das Element in der $n$-ten Zeile sowie $n$-ten Spalte:

$$
\begin{bmatrix}
0 & 0 & \ldots & 0 & x_{n,n}
\end{bmatrix}
\ast 
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
x_{n,n}
\end{bmatrix}
= 0^{2} + 0^{2} + \ldots + x_{n,n}^{2} \stackrel{!}{=} 1 \implies x_{n,n}^{2} = 1
$$

(Aufgrund der nicht gegebenen Nullteilerfreiheit ist $x_{n,n} = \pm 1$ i.a. nicht möglich, somit belassen wir es bei $x_{n,n}^{2} = 1$).
Analoge Aussagen lassen sich für alle Einträge auf der Hauptdiagonalen, also alle $x_{ij} \in A : i = j, \; i,j \in \mathbb{N}$ konstruieren, somit gilt

$$
\forall x_{ij} \in A, \; i = j : x_{ij}^{2} = 1
$$

Für die Einträge oberhalb der Hauptdiagonalen lässt sich durch die Definition der Matrizenmultiplikation erkennen, dass

$$
\begin{bmatrix}
x_{1,1} & x_{1,2} & \ldots & x_{1,n} \\
\end{bmatrix}
\ast
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
x_{n,n}
\end{bmatrix}
= x_{1,1} \cdot 0 + x_{1,2} \cdot 0 + \ldots + x_{1,n} \cdot x_{n,n} \stackrel{!}{=} 0 \implies x_{1,n} = 0
$$

Dies gilt, da wie oben gezeigt, $x_{n,n} = 1$ gilt. Dies gilt für alle Einträge oberhalb der Hauptdiagonalen analog. Somit ist $A$ mindestens eine untere Dreiecksmatrix mit einer Hauptdiagonale bestehend aus Einsen. Unterhalb der Hauptdiagonalen gilt per Definition der oberen Dreiecksmatrix, dass alle Einträge gleich Null sind. Somit ist $A$ eine diagonale Matrix mit 

$$
x_{ij}^{2} = 1, \quad \forall x_{ij} : i = j
$$

***
<br> 

#### Aufgabe 5.4

**(i)**

Um die Äquivalenz zu zeigen, wird der Beweis in Hin- und Rückrichtung aufgeteilt:

`\begin{proof}`
"$\implies$":

Angenommen $D$ ist invertierbar mit

$$
D^{-1} = \begin{bmatrix}
D_{11}^{-1} & 0 \\
0 & D_{22}^{-1}
\end{bmatrix}
$$

So gilt

$$
D \ast D^{-1} =
\begin{bmatrix}
D_{11} & 0 \\
0 & D_{22}
\end{bmatrix}
\ast
\begin{bmatrix}
D_{11}^{-1} & 0 \\
0 & D_{22}^{-1}
\end{bmatrix}
\stackrel{!}{=}
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

Wendet man nun die Matrixmultiplikation an, erhält man die Matrix

$$
\begin{bmatrix}
D_{11} \ast D_{11}^{-1} & 0 \ast 0 \\
0 \ast 0 & D_{22} \ast D_{22}^{-1}
\end{bmatrix}
\stackrel{!}{=} I_{k+l}
$$

Der Eintrag $0$ wird nicht skalar- sondern matrixmultipliziert mit $0$, da er selber wieder eine Matrix in $R^{l,l}$ ist. Gefordert wird durch $I_{k+l}$ eine Matrix, die aus einer Hauptdiagonalen, bestehend aus Einsen besteht, und ansonsten nur aus Nullen besteht. Die Nullmatrizen außerhalb der Hauptdiagonalen bestehend aus quadratischen Matrizen sind bereits existent, somit müssen $D_{11} \ast D_{11}^{-1}$ und $D_{22} \ast D_{22}^{-1}$ Einheitsmatrizen bilden, damit nur auf der Hauptdiagonalen Einsen verbleiben. Dies ist genau dann der Fall, wenn $D_{11}$ und $D_{22}$ invertierbar sind.

"$\impliedby$"

Angenommen $D_{11}$ und $D_{22}$ sind invertierbar, so lassen sich $D_{11}^{-1}$ und $D_{22}^{-1}$ mit 

$$
D_{11} \ast D_{11}^{-1} = I_{k,k} \text{ und } D_{22} \ast D_{22}^{-1} = I_{l,l}
$$

in $R^{k,k}$, respektive $R^{l,l}$ finden. Da $0 \in R^{l,k}$, bzw. $0 \in R^{k,l}$ jeweils Nullmatrizen sind, ihre Elemente also alle gleich Null sind, und $I_{k,k}, I_{l,l}$ jeweils Einheitsmatrizen sind, ist $D$ in diesem Fall eine Einheitsmatrix der Dimension $R^{k+l,k+l}$. Also folgt aus der Invertierbarkeit von $D_{11}$ und $D_{22}$ die Invertierbarkeit von $D$ mit dem inversen Element

$$
D^{-1} = 
\begin{bmatrix}
D_{11}^{-1} & 0 \\
0 & D_{22}^{-1}
\end{bmatrix}
$$

`\end{proof}`

**(ii)**

Es gilt

`\begin{proof}`
$$
\begin{align}
\left[\begin{array}{cc}
I_k & 0 \\
A_{21} A_{11}^{-1} & I_{\ell}
\end{array}\right]\left[\begin{array}{cc}
A_{11} & 0 \\
0 & A_{22}-A_{21} A_{11}^{-1} A_{12}
\end{array}\right]\left[\begin{array}{cc}
I_k & A_{11}^{-1} A_{12} \\
0 & I_{\ell}
\end{array}\right] = & \\ 
\begin{bmatrix}
I_{k}A_{11} & 0 \\
A_{21}A_{11}^{-1}A_{11} & I_{\ell}(A_{22}-A_{21}A_{11}^{-1}A_{12})
\end{bmatrix}
\begin{bmatrix}
I_{k} & A_{11}^{-1}A_{12} \\
0 & I_{\ell}
\end{bmatrix} = & \\
\begin{bmatrix}
I_{k}A_{11}I_{k} & I_{k}A_{11}A_{11}^{-1}A_{12} \\
A_{21}A_{11}^{-1}A_{11}I_{k} & A_{21}I_{k}A_{11}^{-1}A_{12}+A_{22}-A_{21}A_{11}^{-1}A_{12}
\end{bmatrix} = &  \\
\begin{bmatrix}
A_{11} & I_{k}I_{k}A_{12} \\
A_{21}I_{k}I_{k} & A_{22}-A_{21}A_{11}^{-1}A_{12}+A_{21}A_{11}^{-1}A_{12}
\end{bmatrix} = &  \\
 = & \begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix} 
\end{align}
$$

`\end{proof}`
