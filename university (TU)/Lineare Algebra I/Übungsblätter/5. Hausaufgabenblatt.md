***
#### Aufgabe 5.1

**(i)**

Laut Voraussetzung ist $A \in R^{n,n}$ eine Matrix, für die ein $m \in \mathbb{N}$ mit $A^{m} = I_{n}$ existiert. Somit gilt

$$
A^{1} \ast A^{m-1} = I_{n}
$$

Aufgrund von Assoziativität in $R^{n,n}$ und somit Kommutativität von $A$ bezüglich $A$ gilt

$$
A \ast A^{m-1} = \underbrace{ (A) }_{ \text{ $1$ mal } } \ast \underbrace{ (A \ast A \ast \ldots \ast A) }_{ \text{ $m-1$ mal } } = \underbrace{ (A \ast A \ast \ldots \ast A) }_{ \text{ $m-1$ mal } } \ast \underbrace{ (A) }_{ \text{ $1$ mal } } = A^{m-1} \ast A
$$

Somit besitzt $A$ ein identisches Rechts- und Linksinverses, und ist somit mit $A^{m-1}$ invers.

**(ii)**

Die Menge $B \coloneqq \{ A^{k} \mid k \in \mathbb{N} \}$ umfasst die Matrix $A$ sowie ihre Vielfachen. Für die Bestimmung von $\lvert B \rvert$ ist relevant, dass Elemente, die mehrfach in der Menge vorkommen, **nicht** zur Mächtigkeit von $B$ beitragen.
Um zu zeigen, dass sich die Elemente aus $B$ für $k > m$ wiederholen, und somit ignoriert werden können, ist

$$
B = \{ A^{k} \mid k \in \mathbb{N} \} = \{ A^{k} \mid k \in \{ 1,2,\ldots,m \} \} \eqqcolon \hat{B}
$$

zu zeigen. Dafür ist die Inklusion, wie auch die umgekehrte Inklusion zu beweisen.

`\begin{proof}`
"${} \supseteq {}$":

Zu zeigen ist $\{ 1,2,\ldots,m \} \in \mathbb{N}$. Da $m \in \mathbb{N}$ per Voraussetzung gilt, gilt auch die Inklusion trivialerweise.

"${} \subseteq {}$":

Zu zeigen ist $A \in B \implies A \in \hat{B}$. Trivialerweise gilt für $k \leq m$, dass $A \in B \implies A \in \hat{B}$. Für $k > m$ kann $k$ geschrieben werden als

$$
k = xm + i, \; x \in \mathbb{N}, i \in \mathbb{N}, i < m
$$

Somit gilt

$$
\begin{align}
A^{xm + i} & = A^{xm} \ast A^{i} \\
 & = (A^{m})^{x} \ast A^{i} \\
 & = I_{n}^{x} \ast A^{i} \\
 & = I_{n} \ast A^{i} \\
 & = A^{i}
\end{align}
$$

dies gilt, da Vielfache der Einheitsmatrix wieder die Einheitsmatrix ergeben. Somit gilt $A^{i} \in \hat{B}$, da $i < m$, und dies der triviale Fall von oben war.

Somit ist $B = \hat{B}$. Um die Mächtigkeit zu bestimmen, bleibt zu zeigen, dass für alle $A \in B$ keine

$$
A^{l} = A^{k}, \quad l \neq k, \quad l,k \in \{ 1,2,\ldots,m \}
$$

existieren. D.h., es dürfen keine identischen Matrizen für unterschiedliche Anzahlen der Vervielfachung von $A$ in $B$ existieren.

Angenommen, $A^{l} = A^{k}$ für $l \neq k, \; l,k \in \{ 1,2,\ldots,m \}$ gilt. Somit gilt auch

$$
\begin{align}
A^{l} & = A^{k} \\
\Longleftrightarrow A^{m-l} \ast A^{l} & = A^{m-l} \ast A^{k} \\
\Longleftrightarrow A^{m} & = A^{m-l+k} \\
\Longleftrightarrow I_{n} & = A^{m} \ast A^{k - l} \\
\Longleftrightarrow I_{n} & = I_{n} \ast A^{k-l} \\
\Longleftrightarrow I_{n} & = A^{k-l} 
\end{align}
$$

Da $k,l \in \{ 1,2,\ldots,m \}$, gilt **höchstens**

$$
k - l = m-1
$$

Laut Voraussetzung ist $m$ jedoch die kleinste Zahl in $\mathbb{N}$ mit $A^{m} = I_{n}$ ist. Somit besteht ein Widerspruch, und $A^{l} = A^{k} \implies l = k$.

`\end{proof}`
<br> 

Somit gilt abschließend, dass in $B$ keine Matrizen mehrmals vorkommen, und sich diese Matrizen ab $k > m$ wiederholen. Damit gilt

$$
\lvert B \rvert = m
$$

***
#### Aufgabe 5.2

**(i)**

`\begin{proof}`
Sei $p \coloneqq \sum_{i = 0}^{m}\alpha_{i}t^{i}$ und $q \coloneqq \sum_{i = 0}^{n}\beta_{i}t^{i}$. Wir definieren

$$
\hat{p} \coloneqq \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i}, \quad \hat{q} \coloneqq \sum_{i = 0}^{d} \hat{\beta}_{i}t^{i},
$$

wobei $d = \max(m,n)$, und

$$
\hat{\alpha} \coloneqq 
\begin{cases}
\alpha_{i} \quad , i \in \{ 0,1,2,\ldots,m \} \\
0 \quad , \text{ sonst }
\end{cases}, \quad 
\hat{\beta} \coloneqq 
\begin{cases}
\beta_{i} \quad , i \in \{ 0,1,2,\ldots,n \} \\
0 \quad , \text{ sonst }
\end{cases}
$$

Laut Definition der Polynome bezeichnen wir diese als *gleich*, wenn $\hat{\alpha}_{i} = \alpha_{i}$ für $i \in \{ 0,1,\ldots,d \}$ gilt. Für $d = m$ gilt dies laut der Definition von $\hat{\alpha}_{i}$. Für $d = n$ ist $\hat{\alpha}_{i} = 0$ für alle $i > m$. Addition eines Nullsummanden ändert nichts an $\hat{\alpha}_{i}$, somit gilt auch hier $\hat{\alpha}_{i} = \alpha_{i}$. Analog gilt dies auch für ${} \beta_{i}$, und somit gilt $p = \hat{p}$ wie auch $q = \hat{q}$.

Somit kann $p + q$ auch als $\hat{p} + \hat{q}$ geschrieben werden, und es gilt:

$$
\begin{align}
\hat{p} + \hat{q} & = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} + \sum_{i = 0}^{d} \hat{\beta}_{i}t^{i} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} + \hat{\beta}_{i}t^{i} \\
 & = \sum_{i = 0}^{d} t^{i}(\hat{\alpha}_{i}+\hat{\beta}_{i})
\end{align}
$$

Somit gilt für $f_{A}(p+q) = f_{A}(\hat{p} + \hat{q})$:

$$
f_{A}(\hat{p} + \hat{q}) = \sum_{i = 0}^{d} A^{i}(\hat{\alpha}_{i} + \hat{\beta}_{i})
$$

sowie $f_{A}(\hat{p}) = \sum_{i = 0}^{d}\hat{\alpha}_{i}A^{i}$ und $f_{A}(\hat{q}) = \sum_{i = 0}^{d}\hat{\beta}_{i}A^{i}$. Somit gilt für die Addition

$$
\begin{align}
f_{A}(p) + f_{A}(q) = f_{A}(\hat{p}) + f_{A}(\hat{q}) & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} + \sum_{i = 0}^{d} \hat{\beta}_{i}A^{i} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} + \hat{\beta}_{i}A^{i} \\
 & = \sum_{i = 0}^{d} A^{i}(\hat{\alpha}_{i} + \hat{\beta}_{i}) \\
 & = f_{A}(\hat{p} + \hat{q} = f_{A}(p+q)
\end{align}
$$

Für $pq$ gilt

$$
pq = \hat{p}\hat{q} = \sum_{i = 0}^{d} \hat{\alpha}_{i}t^{i} \cdot \sum_{j = 0}^{d} \hat{\beta}_{j}t^{j} = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}t^{i} \hat{\beta}_{j}t^{j}
$$

Somit gilt eingesetzt in unsere Abbildung:

$$
\begin{align}
f_{A}(pq) = f_{A}(\hat{p}\hat{q}) & = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \hat{\beta}_{j}A^{j} \\
 & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \\
 & = f_{A}(\hat{p}) \ast f_{A}(\hat{q}) \\
 & = f_{A}(p) \ast f_{A}(q)
\end{align}
$$

`\end{proof}`
<br> 

**(ii)**

`\begin{proof}`
Seien $B,C \in f_{A}(R[t])$ fest, aber beliebig. Sei $C = f_{A}(p)$ und $B = f_{A}(q)$, somit gilt

$$
\begin{align}
CB = f_{A}(p) \ast f_{A}(q) = f_{A}(\hat{p}) \ast f_{A}(\hat{q}) & = \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \ast \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \\
 & = \sum_{i = 0}^{d} \sum_{j = 0}^{d} \hat{\alpha}_{i}A^{i}\hat{\beta}_{j}A^{j} \\
 & = \sum_{j = 0}^{d} \sum_{i = 0}^{d} \hat{\beta}_{j}A^{j}\hat{\alpha}_{i}A^{i} \\
 & = \sum_{j = 0}^{d} \hat{\beta}_{j}A^{j} \ast \sum_{i = 0}^{d} \hat{\alpha}_{i}A^{i} \\
 & = f_{A}(\hat{q}) \ast f_{A}(\hat{p}) \\
 & = f_{A}(q) \ast f_{A}(p) \\
 & = BC
\end{align}
$$

`\end{proof}`

Damit $f_{A}$ surjektiv ist, muss 

$$
\forall \hat{A} \in R^{n,n} \; \exists \; p \in R[t] : f_{A}(p) = \hat{A}
$$
 gelten. Für $n = 1$ gilt $\hat{A} = [x]$. Nun kann also immer $p$ vom Grad $0$ gewählt werden, so dass

$$
f_{A}(p) = \alpha_{0}A^{0} = \alpha_{0}I_{1}
$$

gilt. Sicherlich ist in $R[t]$ auch ein $p$ mit $\alpha_{0} = x$ enthalten, sodass

$$
\alpha_{0}I_{1} = xI_{1} = [x]
$$

 gilt. $I_{1}$ ist für $1 \times 1$ Matrizen $1$, daher ergibt die Skalarmultiplikation mit $x$ die Matrix, die nur $x$ enthält. Damit ist ${} f_{A}$ für $n = 1$ surjektiv.

Angenommen, $f_{A}$ ist surjektiv für $n \geq 2$, so gilt aus $(ii)$

$$
B,C \in f_{A}(R[t]) \implies B,C \in R^{n,n},
$$

wobei $BC = CB$ gilt. Dies widerspricht jedoch der Nichtkommutativität der Matrizenmultiplikation, welche bekanntlich nur für $n = 1$ nicht gilt. Somit kann $f_{A}$ für $n \geq 2$ nicht surjektiv sein.

***
#### Aufgabe 5.3

